# üè≠ Job Shop Scheduling avec Deep Reinforcement Learning

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.9-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![GitHub](https://img.shields.io/badge/GitHub-Repository-black.svg)](https://github.com/VOTRE_USERNAME/job-shop-rl)

Impl√©mentation compl√®te d'algorithmes de **Deep Reinforcement Learning** pour r√©soudre le **Job Shop Scheduling Problem (JSSP)**, un probl√®me classique d'optimisation combinatoire NP-difficile.

![Gantt Chart Example](results/plots/deep_dqn_FT06_gantt.png)

---

## üìã Table des Mati√®res

- [√Ä Propos](#-√†-propos)
- [Probl√®me du Job Shop](#-probl√®me-du-job-shop)
- [Agents Impl√©ment√©s](#-agents-impl√©ment√©s)
- [Architecture du Projet](#-architecture-du-projet)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [R√©sultats Exp√©rimentaux](#-r√©sultats-exp√©rimentaux)
- [Visualisations](#-visualisations)
- [Benchmarks](#-benchmarks)
- [R√©f√©rences](#-r√©f√©rences)

---

## üéØ √Ä Propos

Ce projet explore l'application du **Deep Reinforcement Learning** pour r√©soudre le Job Shop Scheduling Problem. Il impl√©mente et compare plusieurs approches :

- **Heuristiques classiques** (SPT, LPT, FIFO, etc.)
- **Q-Learning simple** (table-based)
- **Deep Q-Network (DQN)** avec PyTorch
- **Proximal Policy Optimization (PPO)**
- **Deep PPO** avec r√©seaux de neurones

### üéì Objectifs P√©dagogiques

- Comprendre les fondamentaux du Reinforcement Learning
- Impl√©menter des algorithmes DRL de z√©ro
- Appliquer le Deep Learning √† l'optimisation combinatoire
- Benchmarker sur instances classiques (FT06, FT10, LA01, LA02)

---

## üè≠ Probl√®me du Job Shop

### D√©finition

Le **Job Shop Scheduling Problem** consiste √† ordonnancer un ensemble de jobs sur des machines, o√π :

- Chaque **job** comprend plusieurs **op√©rations** √† ex√©cuter dans un ordre sp√©cifique
- Chaque **op√©ration** n√©cessite une **machine** particuli√®re pendant une dur√©e donn√©e
- Chaque **machine** ne peut traiter qu'**une op√©ration √† la fois**
- **Objectif** : Minimiser le **makespan** (temps total pour terminer tous les jobs)

### Exemple : Instance 2√ó2
```
Job 0: Machine 0 (3h) ‚Üí Machine 1 (2h)
Job 1: Machine 1 (2h) ‚Üí Machine 0 (4h)

Solution optimale : Makespan = 7h
```

### Complexit√©

- **Classe** : NP-difficile
- **Espace de solutions** : n! pour n jobs (factorielle)
- **Exemple FT10** (10√ó10) : ~3.6 millions de solutions possibles

---

## ü§ñ Agents Impl√©ment√©s

### 1. **Agents Heuristiques** 

R√®gles de dispatching classiques qui ne n√©cessitent pas d'apprentissage.

#### 1.1 SPT (Shortest Processing Time)
```python
from agents.heuristic_agent import SPTAgent

agent = SPTAgent(instance)
```
- **Principe** : S√©lectionne toujours l'op√©ration avec la dur√©e la plus courte
- **Avantages** : Rapide, d√©terministe
- **Inconv√©nients** : Myope, ne consid√®re pas l'impact global

#### 1.2 LPT (Longest Processing Time)
```python
agent = LPTAgent(instance)
```
- **Principe** : S√©lectionne l'op√©ration la plus longue
- **Usage** : Parfois meilleur pour √©quilibrer les machines

#### 1.3 FIFO (First In First Out)
```python
agent = FIFOAgent(instance)
```
- **Principe** : Premier job disponible en premier
- **Avantages** : Simple, √©quitable

#### 1.4 Most Work Remaining
```python
agent = MostWorkRemainingAgent(instance)
```
- **Principe** : Priorise les jobs avec le plus de temps restant
- **Avantages** : Consid√®re la charge globale

#### 1.5 Random
```python
agent = RandomAgent(instance)
```
- **Usage** : Baseline pour comparaison

### 2. **Q-Learning Simple** 

Q-Learning basique avec table de Q-values.
```python
from agents.dqn_agent import SimpleDQNAgent

agent = SimpleDQNAgent(state_size=5, action_size=3)
```

**Architecture** :
```
√âtat ‚Üí Q-Table (dictionnaire) ‚Üí Q-values
```

**Caract√©ristiques** :
- ‚úÖ Simple √† comprendre et impl√©menter
- ‚úÖ Fonctionne bien sur petites instances (2√ó2, 3√ó3)
- ‚ùå Ne scale pas (explosion m√©moire sur instances > 5√ó5)
- ‚ùå Pas de g√©n√©ralisation (chaque √©tat est unique)

**Fichier** : `src/agents/dqn_agent.py`

---

### 3. **Deep Q-Network (DQN)** 

DQN avec r√©seaux de neurones PyTorch pour approximer la fonction Q.
```python
from agents.deep_dqn_agent import DeepDQNAgent

agent = DeepDQNAgent(
    state_size=state_size,
    action_size=action_size,
    learning_rate=0.001,
    use_double_dqn=True,
    use_dueling=False
)
```

#### Architecture du R√©seau
```
Input (√©tat)
    ‚Üì
Linear(state_size ‚Üí 128) + ReLU
    ‚Üì
Linear(128 ‚Üí 128) + ReLU
    ‚Üì
Linear(128 ‚Üí action_size)
    ‚Üì
Q-values pour chaque action
```

**Param√®tres du r√©seau** : ~17,000 (pour state=7, actions=6)

#### Techniques Avanc√©es Impl√©ment√©es

##### Experience Replay
```python
class ReplayBuffer:
    """Stocke les transitions (s, a, r, s', done)"""
```
- **But** : Casser la corr√©lation temporelle entre exp√©riences
- **Capacit√©** : 100,000 transitions par d√©faut

##### Target Network
```python
self.target_net = copy.deepcopy(self.policy_net)
```
- **But** : Stabiliser l'apprentissage
- **Update** : Toutes les 100 steps

##### Double DQN
```python
# S√©lection avec policy_net, √©valuation avec target_net
next_actions = self.policy_net(next_states).argmax(1)
next_q = self.target_net(next_states).gather(1, next_actions)
```
- **But** : R√©duire le biais de surestimation des Q-values

##### Dueling DQN (optionnel)
```python
Q(s,a) = V(s) + A(s,a) - mean(A(s,¬∑))
```
- **But** : Mieux apprendre la valeur des √©tats

#### Entra√Ænement
```python
from agents.deep_dqn_agent import train_deep_dqn

results = train_deep_dqn(env, agent, num_episodes=1000)
```

**Processus** :
1. **Observation** de l'√©tat
2. **S√©lection d'action** (Œµ-greedy)
3. **Ex√©cution** et observation r√©compense
4. **Stockage** dans replay buffer
5. **Apprentissage** par batch
6. **Mise √† jour** du target network

**Fichier** : `src/agents/deep_dqn_agent.py`

---

### 4. **PPO Simple** 

Proximal Policy Optimization avec tables (version p√©dagogique).
```python
from agents.ppo_agent import SimplePPOAgent

agent = SimplePPOAgent(
    state_size=state_size,
    action_size=action_size,
    clip_epsilon=0.2
)
```

**Diff√©rence avec DQN** :
- Apprend une **politique** (probabilit√©s d'actions) directement
- Utilise **Actor-Critic** (2 r√©seaux)
- **On-policy** (utilise donn√©es r√©centes uniquement)

#### Composants

##### Actor (Politique)
```python
√âtat ‚Üí Probabilit√©s d'actions
```

##### Critic (Fonction de valeur)
```python
√âtat ‚Üí Valeur V(s)
```

##### Advantage Estimation (GAE)
```python
A(s,a) = Q(s,a) - V(s)
```

##### Clipped Surrogate Objective
```python
L = min(ratio √ó A, clip(ratio, 1-Œµ, 1+Œµ) √ó A)
```

**Fichier** : `src/agents/ppo_agent.py`

---

### 5. **Deep PPO** (√Ä venir) 

PPO avec r√©seaux de neurones PyTorch.

---

## üìÅ Architecture du Projet
```
job-shop-rl/
‚îÇ
‚îú‚îÄ‚îÄ README.md                      # Documentation compl√®te
‚îú‚îÄ‚îÄ requirements.txt               # D√©pendances Python
‚îú‚îÄ‚îÄ .gitignore                     # Fichiers √† ignorer
‚îÇ
‚îú‚îÄ‚îÄ src/                          # Code source
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ environment/              # Environnement Job Shop
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ job_shop_instance.py  # Repr√©sentation d'instance
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ solution.py           # Solution et √©valuation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ job_shop_env.py       # Environnement Gymnasium
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ agents/                   # Agents RL
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ heuristic_agent.py    # SPT, LPT, FIFO, etc.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.py          # Q-Learning simple
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deep_dqn_agent.py     # Deep Q-Network (PyTorch)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ppo_agent.py          # PPO simple
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                    # Utilitaires
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visualization.py      # Diagrammes de Gantt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ benchmark_instances.py # Instances FT, LA
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ models/                   # Architectures r√©seaux
‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ examples/                     # Scripts d'utilisation
‚îÇ   ‚îú‚îÄ‚îÄ test_heuristics.py       # Test heuristiques
‚îÇ   ‚îú‚îÄ‚îÄ train_dqn.py             # Entra√Æner DQN simple
‚îÇ   ‚îú‚îÄ‚îÄ train_deep_dqn.py        # Entra√Æner Deep DQN
‚îÇ   ‚îú‚îÄ‚îÄ train_ppo.py             # Entra√Æner PPO
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py              # √âvaluer mod√®le
‚îÇ   ‚îú‚îÄ‚îÄ compare_all.py           # Comparer tous agents
‚îÇ   ‚îú‚îÄ‚îÄ compare_dqn_ppo.py       # DQN vs PPO
‚îÇ   ‚îî‚îÄ‚îÄ benchmark_all_instances.py # Benchmark complet
‚îÇ
‚îú‚îÄ‚îÄ data/                        # Donn√©es (optionnel)
‚îÇ   ‚îî‚îÄ‚îÄ instances/               # Instances suppl√©mentaires
‚îÇ
‚îú‚îÄ‚îÄ results/                     # R√©sultats d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ models/                  # Mod√®les sauvegard√©s
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.pkl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ppo_agent.pkl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deep_dqn_FT06.pth
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ plots/                   # Graphiques
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deep_dqn_FT06_gantt.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deep_dqn_FT06_training.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ benchmark_heuristics.png
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ logs/                    # Logs d'entra√Ænement
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                   # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ exploration.ipynb
‚îÇ
‚îî‚îÄ‚îÄ tests/                       # Tests unitaires
    ‚îî‚îÄ‚îÄ test_environment.py
```

---

## üöÄ Installation

### Pr√©requis

- Python 3.8+
- Git

### √âtapes
```bash
# 1. Cloner le repository
git clone https://github.com/gbencheikh/RL_Job_Shop_Problem
cd job-shop-rl

# 2. Cr√©er un environnement virtuel
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate

# 3. Installer les d√©pendances
pip install --upgrade pip
pip install -r requirements.txt

# 4. V√©rifier l'installation
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
```

### D√©pendances Principales
```txt
torch>=2.0.0              # Deep Learning
gymnasium>=0.29.0         # Environnement RL
matplotlib>=3.7.0         # Visualisation
numpy>=1.24.0            # Calcul num√©rique
pandas>=2.0.0            # Manipulation donn√©es
```

---

## üíª Utilisation

### 1. Tester les Heuristiques
```bash
python examples/test_heuristics.py
```

**Sortie** :
```
√âvaluation des agents (10 √©pisodes chacun):
------------------------------------------------------------

Random:
  Makespan moyen: 12.50
  Meilleur makespan: 10

SPT (Shortest Processing Time):
  Makespan moyen: 8.20
  Meilleur makespan: 8

...
```

### 2. Benchmark sur Instances Classiques
```bash
python examples/benchmark_all_instances.py
```

**G√©n√®re** :
- Tableau comparatif sur FT06, FT10, LA01, LA02
- Graphique de comparaison (`results/plots/benchmark_heuristics.png`)

### 3. Entra√Æner Deep DQN
```bash
python examples/train_deep_dqn.py
```

**Processus** :
```
JOB SHOP avec DEEP Q-LEARNING (PyTorch)
============================================================
Instance: FT06
Agent Deep DQN:
   - Param√®tres: 17283
   - Device: cpu

ENTRA√éNEMENT DEEP DQN
============================================================
Ep   50/1000 | Reward: -120.45 | Makespan:  80.20 | Best:  72.00 | Loss: 2.3456 | Œµ: 0.605
Ep  100/1000 | Reward:  -95.67 | Makespan:  75.80 | Best:  69.00 | Loss: 1.2345 | Œµ: 0.366
...
‚úÖ ENTRA√éNEMENT TERMIN√â
Meilleur makespan: 69.0
```

**G√©n√®re** :
- Mod√®le : `results/models/deep_dqn_FT06.pth`
- Gantt : `results/plots/deep_dqn_FT06_gantt.png`
- Courbe : `results/plots/deep_dqn_FT06_training.png`

### 4. √âvaluer un Mod√®le
```bash
python examples/evaluate.py
```

### 5. Comparer DQN vs PPO
```bash
python examples/compare_dqn_ppo.py
```

---

## üìä R√©sultats Exp√©rimentaux

### Instance FT06 (6 jobs √ó 6 machines)

**Optimal connu : 55**

| M√©thode | Makespan | Gap vs Optimal | Temps Calcul | Fichier |
|---------|----------|----------------|--------------|---------|
| **Optimal** (prouv√©) | **55** | **0.00%** | - | - |
| **Deep DQN** | **69** | **25.45%** | ~15 min | `deep_dqn_agent.py` |
| SPT | 109 | 98.18% | < 1s | `heuristic_agent.py` |
| LPT | 115 | 109.09% | < 1s | `heuristic_agent.py` |
| FIFO | 120 | 118.18% | < 1s | `heuristic_agent.py` |
| Random | 130 | 136.36% | < 1s | `heuristic_agent.py` |

### Am√©lioration Deep DQN vs Heuristiques
```
Deep DQN vs SPT : 36.7% d'am√©lioration
Deep DQN vs LPT : 40.0% d'am√©lioration
Deep DQN vs FIFO: 42.5% d'am√©lioration
```

### Configuration Deep DQN
```python
Architecture : [128, 128] fully connected
Optimizer    : Adam (lr=0.001)
Episodes     : 1000
Batch size   : 64
Replay buffer: 100,000
Target update: Every 100 steps
Techniques   : Double DQN, Experience Replay, Target Network
Device       : CPU
```

---

## üìà Visualisations

### 1. Diagramme de Gantt

![Gantt Chart](results/plots/deep_dqn_FT06_gantt.png)

**Interpr√©tation** :
- Chaque rectangle = une op√©ration
- Couleur = Job
- Axe X = Temps
- Axe Y = Machines
- **Objectif** : Minimiser le makespan (largeur totale)

### 2. Courbe d'Apprentissage

![Training Curve](results/plots/deep_dqn_FT06_training.png)

**Interpr√©tation** :
- Courbe bleue (transparente) = R√©compense brute par √©pisode
- Courbe rouge = Moyenne mobile (50 √©pisodes)
- **Tendance** : Doit augmenter (r√©compense = -makespan, donc moins n√©gatif = mieux)

### 3. Comparaison des Heuristiques

![Heuristics Comparison](results/plots/benchmark_heuristics.png)

**Interpr√©tation** :
- Barres = Gap par rapport √† l'optimal (%)
- Plus bas = meilleur
- Ligne rouge = Optimal (0%)

---

## üèÜ Benchmarks

### Instances Classiques Impl√©ment√©es

| Instance | Taille | Optimal | Source |
|----------|--------|---------|--------|
| FT06 | 6√ó6 | 55 | Fisher & Thompson (1963) |
| FT10 | 10√ó10 | 930 | Fisher & Thompson (1963) |
| FT20 | 20√ó5 | 1165 | Fisher & Thompson (1963) |
| LA01 | 10√ó5 | 666 | Lawrence (1984) |
| LA02 | 10√ó5 | 655 | Lawrence (1984) |

### R√©sultats Complets
```bash
python examples/benchmark_all_instances.py
```

**Tableau g√©n√©r√©** :
```
Instance  | SPT    | Deep DQN | Gap DQN
----------|--------|----------|--------
FT06      | 109    | 69       | 25.45%
FT10      | 1250   | 1050     | 12.90%
LA01      | 850    | 720      | 8.11%
...
```

---

## üõ†Ô∏è Scripts Disponibles

### Entra√Ænement

| Script | Description | Dur√©e | Sortie |
|--------|-------------|-------|--------|
| `train_dqn.py` | Q-Learning simple | 5 min | `dqn_agent.pkl` |
| `train_deep_dqn.py` | Deep DQN PyTorch | 15 min | `deep_dqn_*.pth` |
| `train_ppo.py` | PPO simple | 10 min | `ppo_agent.pkl` |

### √âvaluation

| Script | Description | Sortie |
|--------|-------------|--------|
| `evaluate.py` | √âvalue un mod√®le sauvegard√© | Stats + Gantt |
| `benchmark_all_instances.py` | Test sur toutes instances | Tableau + graphique |
| `compare_all.py` | Compare toutes heuristiques | Classement |
| `compare_dqn_ppo.py` | DQN vs PPO | Boxplots |

---

## üß™ Cr√©er une Nouvelle Instance
```python
from environment.job_shop_instance import JobShopInstance

# M√©thode 1 : Depuis des donn√©es
jobs = [
    [(0, 3), (1, 2), (2, 2)],  # Job 0: M0(3h) ‚Üí M1(2h) ‚Üí M2(2h)
    [(1, 2), (2, 4), (0, 1)]   # Job 1: M1(2h) ‚Üí M2(4h) ‚Üí M0(1h)
]
instance = JobShopInstance(jobs)

# M√©thode 2 : Al√©atoire
instance = JobShopInstance.create_random_instance(
    num_jobs=5,
    num_machines=5,
    min_duration=1,
    max_duration=10
)

# M√©thode 3 : Depuis un fichier
instance = JobShopInstance.load_from_file('data/instances/custom.txt')
```

---

## üî¨ Exp√©rimentations Avanc√©es

### Optimiser les Hyperparam√®tres
```python
# Tester diff√©rentes configurations
configs = {
    'learning_rate': [0.0001, 0.001, 0.01],
    'batch_size': [32, 64, 128],
    'epsilon_decay': [0.995, 0.998, 0.999]
}

for lr in configs['learning_rate']:
    for bs in configs['batch_size']:
        agent = DeepDQNAgent(
            state_size=state_size,
            action_size=action_size,
            learning_rate=lr,
            batch_size=bs
        )
        results = train_deep_dqn(env, agent, num_episodes=500)
        # Sauvegarder r√©sultats
```

### Utiliser Dueling DQN
```python
agent = DeepDQNAgent(
    state_size=state_size,
    action_size=action_size,
    use_dueling=True  # Active Dueling architecture
)
```

---

## üìö R√©f√©rences

### Papers

1. **DQN Original**
   - Mnih et al. (2015) - "Human-level control through deep reinforcement learning"
   - Nature 518, 529‚Äì533

2. **Double DQN**
   - van Hasselt et al. (2016) - "Deep Reinforcement Learning with Double Q-learning"
   - AAAI 2016

3. **Dueling DQN**
   - Wang et al. (2016) - "Dueling Network Architectures for Deep Reinforcement Learning"
   - ICML 2016

4. **PPO**
   - Schulman et al. (2017) - "Proximal Policy Optimization Algorithms"
   - arXiv:1707.06347

5. **Job Shop Scheduling**
   - Fisher & Thompson (1963) - "Probabilistic Learning Combinations of Local Job-Shop Scheduling Rules"

### Ressources

- [Sutton & Barto - Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html)
- [OpenAI Spinning Up](https://spinningup.openai.com/)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [Gymnasium Documentation](https://gymnasium.farama.org/)

---

## ü§ù Contribution

Les contributions sont les bienvenues ! Pour contribuer :

1. Fork le projet
2. Cr√©er une branche (`git checkout -b feature/AmazingFeature`)
3. Commit vos changements (`git commit -m 'Add AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

---

## üìù License

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

---

## üë®‚Äçüíª Auteur

**BENCHEIKH Ghita**

- GitHub: [@gbencheikh](https://github.com/gbencheikh)
- LinkedIn: [Ghita BENCHEIKH](https://www.linkedin.com/in/ghita-bencheikh/)

---

## üôè Remerciements

- Communaut√© PyTorch pour les outils Deep Learning
- OpenAI Gym/Gymnasium pour l'infrastructure RL
- Fisher & Thompson pour les instances benchmark classiques
- Tous les contributeurs et chercheurs en Reinforcement Learning

---

## üìä Statistiques du Projet
```bash
# Lignes de code
find src -name "*.py" | xargs wc -l

# Nombre de commits
git rev-list --count HEAD

# Nombre de fichiers
find . -type f -name "*.py" | wc -l
```

**R√©sultat estim√©** :
- ~3000 lignes de code Python
- 8 agents diff√©rents impl√©ment√©s
- 5 instances benchmark
- 10+ scripts d'exemple

---

## üó∫Ô∏è Roadmap

### ‚úÖ Phase 1 - Compl√©t√©e
- [x] Environnement Gymnasium Job Shop
- [x] Agents heuristiques (SPT, LPT, FIFO, etc.)
- [x] Q-Learning simple
- [x] Deep DQN avec PyTorch
- [x] PPO simple
- [x] Instances benchmark (FT, LA)
- [x] Visualisations Gantt

### üöß Phase 2 - En Cours
- [ ] Deep PPO avec PyTorch
- [ ] Prioritized Experience Replay
- [ ] Noisy Networks
- [ ] Rainbow DQN

### üìÖ Phase 3 - √Ä Venir
- [ ] Graph Neural Networks pour Job Shop
- [ ] Attention Mechanisms
- [ ] Pointer Networks
- [ ] Transfer Learning entre instances
- [ ] Multi-agent RL
- [ ] Interface web interactive

---

## üìû Support

Pour toute question ou probl√®me :

1. Consultez la [documentation](#)
2. Ouvrez une [issue](https://github.com/gbencheikh/RL_Job_Shop_Problem/issues)
3. Contactez-moi par [email](ghita.bencheikh@gmail.com)

---

<div align="center">

**‚≠ê Si ce projet vous a √©t√© utile, n'h√©sitez pas √† lui donner une √©toile ! ‚≠ê**

Made with ‚ù§Ô∏è and üß† by BENCHEIKH Ghita

</div>