# Job Shop Scheduling avec Reinforcement Learning

## ğŸ“‹ Description

Ce projet implÃ©mente un systÃ¨me de Reinforcement Learning pour rÃ©soudre le **Job Shop Scheduling Problem (JSSP)** en utilisant des algorithmes comme DQN et PPO.

### Qu'est-ce que le Job Shop Scheduling ?

Le Job Shop Scheduling est un problÃ¨me d'optimisation combinatoire oÃ¹ :
- **n jobs** doivent Ãªtre traitÃ©s
- Chaque job a **m opÃ©rations** Ã  effectuer dans un ordre spÃ©cifique
- Chaque opÃ©ration nÃ©cessite une **machine particuliÃ¨re** pendant un certain temps
- Chaque machine ne peut traiter qu'**une opÃ©ration Ã  la fois**
- **Objectif** : Minimiser le temps total (makespan) pour complÃ©ter tous les jobs

### Exemple Simple (2 jobs Ã— 2 machines)
```
Job 1: M1(3h) â†’ M2(2h)
Job 2: M2(2h) â†’ M1(4h)

Solution optimale : Makespan = 7h
```

## ğŸ¯ Objectifs du Projet

1. âœ… CrÃ©er un environnement Gymnasium compatible pour Job Shop
2. âœ… ImplÃ©menter des agents RL (DQN, PPO)
3. âœ… Visualiser les solutions avec des diagrammes de Gantt
4. âœ… Tester sur des benchmarks classiques (FT06, FT10, etc.)
5. âœ… Comparer avec des heuristiques classiques

## ğŸ› ï¸ Technologies

- **Python 3.12**
- **PyTorch** - Deep Learning
- **Gymnasium** - Environnement RL
- **Stable-Baselines3** - Algorithmes RL state-of-the-art
- **Matplotlib/Plotly** - Visualisation

## ğŸ“ Structure du Projet
```
job-shop-rl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/       # Environnement Job Shop
â”‚   â”œâ”€â”€ agents/           # Agents RL (DQN, PPO)
â”‚   â”œâ”€â”€ utils/            # Visualisation, logging
â”‚   â””â”€â”€ models/           # Architectures de rÃ©seaux
â”œâ”€â”€ examples/             # Scripts d'entraÃ®nement
â”œâ”€â”€ data/instances/       # Instances de test
â””â”€â”€ results/              # ModÃ¨les et rÃ©sultats
```

## ğŸš€ Installation
```bash
# Cloner le repository
git clone <votre-repo>
cd job-shop-rl

# CrÃ©er l'environnement virtuel
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Installer les dÃ©pendances
pip install -r requirements.txt
```

## ğŸ“Š Utilisation

### EntraÃ®ner un agent sur une instance simple
```bash
python examples/train_simple.py
```

### EntraÃ®ner sur des benchmarks
```bash
python examples/train_benchmark.py --instance FT06
```

### Ã‰valuer un modÃ¨le
```bash
python examples/evaluate.py --model results/models/best_model.pth
```

## ğŸ“ˆ RÃ©sultats Attendus

- Graphiques de convergence de l'apprentissage
- Diagrammes de Gantt des solutions trouvÃ©es
- Comparaison avec heuristiques classiques (SPT, LPT, etc.)
- Temps de calcul et qualitÃ© des solutions

## ğŸ“ Concepts ClÃ©s - Reinforcement Learning

### Ã‰tat (State)
- OpÃ©rations dÃ©jÃ  ordonnancÃ©es
- Machines disponibles
- Temps courant
- OpÃ©rations restantes

### Action
- Choisir la prochaine opÃ©ration Ã  ordonnancer

### RÃ©compense
- RÃ©compense nÃ©gative = makespan (on veut minimiser)
- Bonus si on atteint un bon ordonnancement
- PÃ©nalitÃ© si contraintes violÃ©es

## ğŸ“š RÃ©fÃ©rences

- Fisher & Thompson (1963) - Instances FT
- Taillard (1993) - Instances benchmarks
- Sutton & Barto - Reinforcement Learning: An Introduction

## ğŸ‘¨â€ğŸ’» Auteur

[Votre Nom]

## ğŸ“ License

MIT