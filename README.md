# Job Shop Scheduling avec Reinforcement Learning

## ğŸ“‹ Description

Ce projet implÃ©mente un systÃ¨me de Reinforcement Learning pour rÃ©soudre le **Job Shop Scheduling Problem (JSSP)** en utilisant des algorithmes comme DQN et PPO.

### Qu'est-ce que le Job Shop Scheduling ?

Le Job Shop Scheduling est un problÃ¨me d'optimisation combinatoire oÃ¹ :
- **n jobs** doivent Ãªtre traitÃ©s
- Chaque job a **m opÃ©rations** Ã  effectuer dans un ordre spÃ©cifique
- Chaque opÃ©ration nÃ©cessite une **machine particuliÃ¨re** pendant un certain temps
- Chaque machine ne peut traiter qu'**une opÃ©ration Ã  la fois**
- **Objectif** : Minimiser le temps total (makespan) pour complÃ©ter tous les jobs

### Exemple Simple (2 jobs Ã— 2 machines)
```
Job 1: M1(3h) â†’ M2(2h)
Job 2: M2(2h) â†’ M1(4h)

Solution optimale : Makespan = 7h
```

## ğŸ¯ Objectifs du Projet

1. [x] CrÃ©er un environnement Gymnasium compatible pour Job Shop
2. [x] ImplÃ©menter l'agent RL : DQN
3. [x] Visualiser les solutions avec des diagrammes de Gantt
4. [x] Tester sur des benchmarks classiques (FT06, FT10, etc.)
5. [x] Comparer avec des heuristiques classiques

## ğŸ› ï¸ Technologies

- **Python 3.12**
- **PyTorch** - Deep Learning
- **Gymnasium** - Environnement RL
- **Stable-Baselines3** - Algorithmes RL state-of-the-art
- **Matplotlib/Plotly** - Visualisation

## ğŸ“ Structure du Projet
```
job-shop-rl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/      # Environnement Job Shop
â”‚   â”œâ”€â”€ agents/           # Agents RL (Heuristic, DQN, PPO)
â”‚   â”œâ”€â”€ utils/            # Visualisation, logging
â”‚   â””â”€â”€ models/           # Architectures de rÃ©seaux
â”œâ”€â”€ examples/             # Scripts d'entraÃ®nement
â””â”€â”€ results/              # ModÃ¨les et rÃ©sultats
```

## ğŸš€ Installation
```bash
# Cloner le repository
git clone https://github.com/gbencheikh/RL_Job_Shop_Problem
cd job-shop-rl

# CrÃ©er l'environnement virtuel
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Installer les dÃ©pendances
pip install -r requirements.txt
```

## ğŸ“Š Utilisation

### EntraÃ®ner un agent sur une instance simple
```bash
python examples/train_DQN_agent.py
```

### EntraÃ®ner sur des benchmarks
```bash
python examples/train_benchmark.py --instance FT06
```

### Ã‰valuer un modÃ¨le
```bash
python examples/evaluate.py --model results/models/best_model.pth
```

## ğŸ“ˆ RÃ©sultats Attendus

- Graphiques de convergence de l'apprentissage
- Diagrammes de Gantt des solutions trouvÃ©es
- Comparaison avec heuristiques classiques (SPT, LPT, etc.)
- Temps de calcul et qualitÃ© des solutions

## Concepts ClÃ©s - Reinforcement Learning

### Ã‰tat (State)
- OpÃ©rations dÃ©jÃ  ordonnancÃ©es
- Machines disponibles
- Temps courant
- OpÃ©rations restantes

### Action
- Choisir la prochaine opÃ©ration Ã  ordonnancer

### RÃ©compense
- RÃ©compense nÃ©gative = makespan (on veut minimiser)
- Bonus si on atteint un bon ordonnancement
- PÃ©nalitÃ© si contraintes violÃ©es

## ğŸ“š RÃ©fÃ©rences

- Fisher & Thompson (1963) - Instances FT
- Taillard (1993) - Instances benchmarks
- Sutton & Barto - Reinforcement Learning: An Introduction

## ğŸ‘¨â€ğŸ’» Auteur

Ghita BENCHEIKH

## ğŸ“Š RÃ©sultats ExpÃ©rimentaux

### Instance FT06 (6 jobs Ã— 6 machines, optimal = 55)

| MÃ©thode | Makespan | Gap vs Optimal | Temps Calcul |
|---------|----------|----------------|--------------|
| Optimal (prouvÃ©) | 55 | 0.00% | - |
| **Deep DQN** | **69** | **25.45%** | ~15 min |
| SPT Heuristic | 109 | 98.18% | < 1s |

### Performance Deep DQN
- Architecture: 128Ã—128 rÃ©seau de neurones
- EntraÃ®nement: 1000 Ã©pisodes
- Device: CPU (PyTorch 2.9.0)
- **AmÃ©lioration vs SPT: 36.7%**

## ğŸš€ Utilisation

### EntraÃ®ner Deep DQN sur FT06
```bash
python examples/train_deep_dqn.py
```

### Benchmark toutes les heuristiques
```bash
python examples/benchmark_all_instances.py
```
## ğŸ“ License

MIT